{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <h1><center>CMSC 478: Introduction to Machine Learning</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Clustering, GMM, MLE, and Bayesian Models</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Agenda</center></h1>\n",
    "\n",
    "- <b>Unsupervised Learning</b>\n",
    "    - Clustering\n",
    "        - K-Means\n",
    "        - DBSCAN\n",
    "        - Agglomerative Clustering\n",
    "    - Clustering for Image Segmentation\n",
    "    - Clustering for Preprocessing and Semi-Supervised Learning\n",
    "- <b>Gaussian Mixture Models (GMM)</b>\n",
    "- <b>Maximum Likelihood Estimation (MLE)</b>\n",
    "- <b>Bayes' Theorem and Bayesian Models</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Unsupervised Learning Techniques</center></h1>\n",
    "\n",
    "Here are a few Unsupervised Learning techniques applicable on unlabeled data:\n",
    "\n",
    "- Dimensionality Reduction: Projecting high-dimensional data into spaces with lower dimensionality.\n",
    "\n",
    "\n",
    "- Clustering: Grouping similar instances together into clusters.\n",
    "\n",
    "\n",
    "- Anomaly Detection: Learning what **normal** data looks like, and then use that to detect abnormal instances, such as defective items on a production line or a new trend in a time series.\n",
    "\n",
    "\n",
    "- Density Estimation: Estimating the Probability Density Function (PDF) of the random process that generated the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Clustering</center></h1>\n",
    "\n",
    "- The goal is to group similar instances together into clusters.\n",
    "\n",
    "\n",
    "- Clustering is a great tool for data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, dimensionality reduction, and more.\n",
    "\n",
    "\n",
    "- Just like in classification, each instance gets assigned to a group. However, unlike classification, clustering is an **unsupervised** task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Clustering</center></h1>\n",
    "\n",
    "<center><img src=\"img/clustering-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[4]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Similarity</center></h1>\n",
    "\n",
    "<center><img src=\"img/similarity.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[4]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Clustering Approaches</center></h1>\n",
    "\n",
    "<center><img src=\"img/clustering-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means</center></h1>\n",
    "\n",
    "<center><img src=\"img/k-means-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means</center></h1>\n",
    "\n",
    "<center><img src=\"img/k-means-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means</center></h1>\n",
    "\n",
    "<center><img src=\"img/k-means-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means</center></h1>\n",
    "\n",
    "<center><img src=\"img/k-means-4.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means</center></h1>\n",
    "\n",
    "<center><img src=\"img/k-means-5.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means</center></h1>\n",
    "\n",
    "<center><img src=\"img/k-means-ex-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means Example</center></h1>\n",
    "\n",
    "<center><img src=\"img/k-means-ex-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means Example</center></h1>\n",
    "\n",
    "<center><img src=\"img/k-means-ex-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means Example</center></h1>\n",
    "\n",
    "<center><img src=\"img/k-means-ex-4.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means Example</center></h1>\n",
    "\n",
    "<center><img src=\"img/k-means-ex-5.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means Demo</center></h1>\n",
    "\n",
    "https://www.naftaliharris.com/blog/visualizing-k-means-clustering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Centroid Initialization</center></h1>\n",
    "\n",
    "- Although the K-Means algorithm is guaranteed to converge, it may not converge to the right solution (i.e., it may converge to a local optimum).\n",
    "\n",
    "\n",
    "- Whether K-Means converges to the right solution or not depends on the centroid initialization.\n",
    "\n",
    "<center><img src=\"img/centroid.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Centroid Initialization and Performance Metric</center></h1>\n",
    "\n",
    "- A solution to the issue with centroid initializationis is to run the algorithm multiple times with different random initializations and keep the best solution.\n",
    "\n",
    "\n",
    "- However, the question would be how exactly we can know which solution is the best.\n",
    "\n",
    "\n",
    "- That is why a **performance metric** is needed to evaluate clustering models!\n",
    "\n",
    "\n",
    "- One metric to evaluate clustering model is called the modelâ€™s **inertia**, which is the **mean squared distance** between each instance and its closest centroid. The best model is the the model with the lowest **inertia**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means ++</center></h1>\n",
    "\n",
    "- An important improvement to the K-Means algorithm, **K-Means++**, was proposed in a 2006 paper by David Arthur and Sergei Vassilvitskii.\n",
    "\n",
    "\n",
    "- They introduced a smarter initialization step that tends to select centroids that are distant from one another.\n",
    "\n",
    "\n",
    "- This improvement makes the K-Means algorithm much less likely to converge to a suboptimal solution.\n",
    "\n",
    "\n",
    "- The `KMeans` class of scikit-learn uses this initialization method by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Accelerated K-Means and Mini-Batch K-Means</center></h1>\n",
    "\n",
    "- Another important improvement to the K-Means algorithm was proposed in a 2003 paper by Charles Elkan, called **Accelerated K-Means**. This is the algorithm the `KMeans` class of scikit-learn uses by default.\n",
    "\n",
    "\n",
    "- **Accelerated K-Means** considerably accelerates the algorithm by avoiding many unnecessary distance calculations.\n",
    "\n",
    "\n",
    "- Yet another important variant of the K-Means algorithm was proposed in a 2010 paper by David Sculley, called **Mini-Batch K-Means**. Instead of using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration.\n",
    "\n",
    "\n",
    "- **Mini-batch K-Means** speeds up the algorithm typically by a factor of three or four and makes it possible to cluster huge datasets that do not fit in memory.\n",
    "\n",
    "\n",
    "- Scikit-Learn implements this algorithm in the `MiniBatchKMeans` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Mini-Batch K-Means vs K-Means</center></h1>\n",
    "\n",
    "\n",
    "<center><img src=\"img/mini-batch.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Finding the Optimal Number of Clusters</center></h1>\n",
    "\n",
    "<center><img src=\"img/optimal-number.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Silhouette Score</center></h1>\n",
    "\n",
    "- A precise approach (but also computationally expensive) to find the optimal number of clusters is to use the **silhouette score**, which is the mean **silhouette coefficient** over all the instances.\n",
    "\n",
    "\n",
    "- An instanceâ€™s **silhouette coefficient** is equal to $\\frac{(b â€“ a)} {max(a, b)}$ where a is the mean distance to the other instances in the same cluster (i.e., the mean intra-cluster distance) and b is the mean nearest-cluster distance i.e., the mean distance to the instances of the next closest cluster, defined as the one that minimizes b, excluding the instanceâ€™s own cluster.\n",
    "\n",
    "\n",
    "- The **silhouette coefficient** can vary between â€“1 and +1.\n",
    "\n",
    "\n",
    "- A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to â€“1 means that the instance may have been assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Silhouette Score</center></h1>\n",
    "\n",
    "<center><img src=\"img/silhouette.svg\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[6]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Silhouette Diagram</center></h1>\n",
    "\n",
    "- The knife shapeâ€™s height indicates the number of instances the cluster contains, and its width represents the sorted silhouette coefficients of the instances in the cluster (wider is better). The dashed line indicates the mean silhouette coefficient.\n",
    "\n",
    "<center><img src=\"img/silhouette-diagram.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>K-Means Limits</center></h1>\n",
    "\n",
    "- Despite its many merits, most notably being fast and scalable, K-Means is not perfect:\n",
    "\n",
    "    - It is necessary to run the algorithm several times to avoid suboptimal solutions.\n",
    "    - You need to specify the number of clusters, which can be quite a hassle.\n",
    "    - K-Means does not behave very well when the clusters have varying sizes, different densities, or nonspherical shapes.\n",
    "    - It is important to scale the input features before you run K-Means, or the clusters may be very stretched and K-Means will perform poorly. Scaling the features does not guarantee that all the clusters will be nice and spherical, but it generally improves things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Image Segmentation</center></h1>\n",
    "\n",
    "<center><img src=\"img/image-segmentation-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[7]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Clustering for Image Segmentation</center></h1>\n",
    "\n",
    "<center><img src=\"img/image-segmentation-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Clustering for Image Segmentation</center></h1>\n",
    "\n",
    "<center><img src=\"img/image-segmentation-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Clustering for Image Segmentation</center></h1>\n",
    "\n",
    "<center><img src=\"img/image-segmentation-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Clustering for Preprocessing and Semi-Supervised Learning</center></h1>\n",
    "\n",
    "- Clustering can be used as a preprocessing step for dimensionality reduction (from original D to K) before applying a classification method.\n",
    "\n",
    "\n",
    "- Moreover, for partially labellled data, clustering can be used for **label propagation**, i.e. assigning the same label for all data points in the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Active Learning</center></h1>\n",
    "\n",
    "- To improve semi-supervised learning using clustering, the next step could be to do a few rounds of **active learning**, which is when a human expert interacts with the learning algorithm, providing labels for specific instances when the algorithm requests them.\n",
    "\n",
    "\n",
    "- There are many different strategies for **active learning**, but one of the most common ones is called **uncertainty sampling**. Here is how it works:\n",
    "    1. The model is trained on the labeled instances gathered so far, and this model is used to make predictions on all the unlabeled instances.\n",
    "    2. The instances for which the model is most uncertain (i.e., when its estimated probability is lowest) are given to the expert to be labeled.\n",
    "    3. You iterate this process until the performance improvement stops being worth the labeling effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>DBSCAN</center></h1>\n",
    "\n",
    "- This algorithm defines clusters as continuous regions of high density. Here is how it works:\n",
    "    - For each instance, the algorithm counts how many instances are located within a small distance **Îµ** (epsilon) from it. This region is called the instanceâ€™s **Îµ-neighborhood**.\n",
    "    - If an instance has at least `min_samples` instances in its **Îµ-neighborhood** (including itself), then it is considered a **core instance**. In other words, **core instances** are those that are located in dense regions.\n",
    "    - All instances in the neighborhood of a **core instance** belong to the same cluster. This neighborhood may include other **core instances**; therefore, a long sequence of neighboring **core instances** forms a single cluster.\n",
    "    - Any instance that is not a **core instance** and does not have one in its neighborhood is considered an **anomaly**.\n",
    "    \n",
    "    \n",
    "- **DBSCAN** works well if all the clusters are dense enough and if they are well separated by low-density regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>DBSCAN</center></h1>\n",
    "\n",
    "<center><img src=\"img/dbscan.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>DBSCAN</center></h1>\n",
    "\n",
    "- DBSCAN is a very simple yet powerful algorithm capable of identifying any number of clusters of any shape.\n",
    "\n",
    "\n",
    "- It is robust to outliers, and it has just two hyperparameters ( `eps` and `min_samples` ).\n",
    "\n",
    "\n",
    "- If the density varies significantly across the clusters, however, it can be impossible for it to capture all the clusters properly.\n",
    "\n",
    "\n",
    "- Its computational complexity is roughly $O(m \\log m)$, making it pretty close to linear with regard to the number of instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hierarchical Clustering - Agglomerative Clustering</center></h1>\n",
    "\n",
    "<center><img src=\"img/agglo-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hierarchical Clustering - Agglomerative Clustering</center></h1>\n",
    "\n",
    "<center><img src=\"img/agglo-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hierarchical Clustering - Agglomerative Clustering</center></h1>\n",
    "\n",
    "<center><img src=\"img/agglo-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Gaussian Mixture Model (GMM)</center></h1>\n",
    "\n",
    "- A Gaussian mixture model (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several **Gaussian** distributions whose parameters are unknown.\n",
    "\n",
    "<center><img src=\"img/gmm-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[8]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Gaussian Distribution</center></h1>\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2}\\frac{||x - \\mu||^2}{\\sigma^2}}\n",
    "$$\n",
    "\n",
    "where $\\mu$ is mean and\n",
    "$\\sigma$ is standard deviation, and thus variance is $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gaussian (Normal) Distribution\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "xs = np.linspace(-5,15,1000)\n",
    "mu = 5.5\n",
    "\n",
    "plt.plot(xs, np.exp(-0.1 * (xs-mu)**2))\n",
    "plt.plot([mu, mu], [0, 1], 'r--',lw=3)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$p(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Gaussian Distribution</center></h1>\n",
    "\n",
    "- All the instances generated from a single **Gaussian** distribution form a cluster that typically looks like an **ellipsoid**.\n",
    "\n",
    "<center><img src=\"img/ellipsoid.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[9]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb41b72f48a4db28b66434a58086057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=501, description='N', max=1000, min=2), FloatSlider(value=0.0, descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ipywidgets import interact\n",
    "@interact(N=(2,1000), mux=(-10,10,0.1), muy=(-10,10,0.1), sigmax=(0.1,20,0.1), sigmay=(0.1,20,0.1), sigmaxy=(-10,10,0.1))\n",
    "\n",
    "def bunchOfData(N, mux, muy, sigmax, sigmay, sigmaxy):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    xs = np.random.normal(0.0, 1.0, N)\n",
    "    ys = np.random.normal(0.0, 1.0, N)\n",
    "    X = np.vstack((xs,ys)).T\n",
    "    L = np.linalg.cholesky(np.array([[sigmax, sigmaxy],[sigmaxy, sigmay]]))\n",
    "    newX = np.dot(X,L.T) + [mux,muy]\n",
    "    plt.plot(newX[:,0], newX[:,1], 'bo', alpha=0.8);\n",
    "    plt.xlim(-15,15)\n",
    "    plt.ylim(-15,15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GMM Example</center></h1>\n",
    "\n",
    "<center><img src=\"img/uw-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[10]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GMM Example Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/uw-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[10]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GMM Example Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/uw-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[10]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GMM Example Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/uw-4.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[10]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GMM Example Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/uw-5.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[10]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GMM Example Cont. - 3D Projection</center></h1>\n",
    "\n",
    "<center><img src=\"img/uw-7.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[10]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GMM 3D</center></h1>\n",
    "\n",
    "<center><img src=\"img/gmm-3d.gif\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[11]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Gaussian 3D to 2D Projection</center></h1>\n",
    "\n",
    "<center><img src=\"img/gmm-3d-2d.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[12]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GMM Example Cont. - Prior Probability</center></h1>\n",
    "\n",
    "<center><img src=\"img/uw-8-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<center><img src=\"img/uw-8-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[10]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GMM Example Cont. - Likelihood</center></h1>\n",
    "\n",
    "<center><img src=\"img/uw-9-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<center><img src=\"img/uw-9-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[10]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GMM Diagram</center></h1>\n",
    "\n",
    "<center><img src=\"img/gmm-diagram.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GMM and Expectation Maximization (EM) Algorithm</center></h1>\n",
    "\n",
    "- Expectation Maximization (EM) algorithm attempts estimating the weights $\\phi$ and all the distribution parameters $\\mu^{(1)}$ to $\\mu^{(k)}$ and $\\sum^{(1)}$ to $\\sum^{(k)}$ given the dataset X.\n",
    "\n",
    "\n",
    "- EM has many similarities with the K-Means algorithm. It initializes the cluster parameters randomly, then it repeats two steps until convergence:\n",
    "    - First, assigning instances to clusters - this is called the **expectation step**.\n",
    "    - Then, updating the clusters - this is called the **maximization step**.\n",
    "    \n",
    "    \n",
    "- In the context of clustering, you can think of EM as a generalization of K-Means that not only finds the cluster centers $\\mu^{(1)}$ to $\\mu^{(k)}$, but also their size, shape, and orientation $\\sum^{(1)}$ to $\\sum^{(k)}$â€‹, as well as their relative weights $\\phi^{(1)}$ to $\\phi^{(k)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>EM Algorithm Cont.</center></h1>\n",
    "\n",
    "- Unlike K-Means, EM uses **soft** cluster assignments, not **hard** assignments.\n",
    "\n",
    "\n",
    "- For each instance, during the **expectation step**, the algorithm estimates the probability that it belongs to each cluster (based on the current cluster parameters).\n",
    "\n",
    "\n",
    "- Then, during the **maximization step**, each cluster is updated using all the instances in the dataset, with each instance weighted by the estimated probability that it belongs to that cluster.\n",
    "\n",
    "\n",
    "- These probabilities are called the **responsibilities** of the clusters for the instances.\n",
    "\n",
    "\n",
    "- During the **maximization step**, each clusterâ€™s update will mostly be impacted by the instances it is most **responsible** for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Likelihood Function</center></h1>\n",
    "\n",
    "- Given a statistical model with some parameters Î¸, the word **probability** is used to describe how plausible a future outcome x is (knowing the parameter values Î¸), while the word **likelihood** is used to describe how plausible a particular set of parameter values Î¸ are, after the outcome x is known.\n",
    "\n",
    "<center><img src=\"img/likelihood-function.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Maximum Likelihood Estimation (MLE)</center></h1>\n",
    "\n",
    "- Given a dataset X, a common task is to try to estimate the most likely values for the model parameters.\n",
    "\n",
    "\n",
    "- To do this, you must find the values that maximize the **likelihood function**, given X.\n",
    "\n",
    "\n",
    "- In the example shown in the previous slide, if you have observed a single instance x=2.5, the **Maximum Likelihood Estimate (MLE)** of Î¸ is Î¸ =1.5\n",
    "\n",
    "\n",
    "- If a prior probability distribution g over Î¸ exists, it is possible to take it into account by maximizing L(Î¸|x)g(Î¸) rather than just maximizing L(Î¸|x). This is called **Maximum A-Posteriori (MAP)** estimation.\n",
    "\n",
    "\n",
    "- Since MAP constrains the parameter values, you can think of it as a regularized version of MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>BIC and AIC</center></h1>\n",
    "\n",
    "- With K-Means, you could use the **inertia** or the **silhouette score** to select the appropriate number of clusters. But with Gaussian mixtures, it is not possible to use these metrics because they are not reliable when the clusters are not spherical or have different sizes.\n",
    "\n",
    "\n",
    "- Instead, you can try to find the model that minimizes a theoretical information criterion, such as the Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC)\n",
    "\n",
    "\n",
    "<center><img src=\"img/bic.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Equation from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Bayes Theorem</center></h1>\n",
    "\n",
    "<center><img src=\"img/bayes.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Equation from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Bayesian GMM </center></h1>\n",
    "\n",
    "- Rather than manually searching for the optimal number of clusters, you can use the `BayesianGaussianMixture` class, which is capable of giving weights equal (or close) to zero to unnecessary clusters.\n",
    "\n",
    "\n",
    "- Set the number of clusters `n_components` to a value that you have good reason to believe is greater than the optimal number of clusters (this assumes some minimal knowledge about the problem at hand), and the algorithm will eliminate the unnecessary clusters automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Coming Up Next: Deep Learning</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>References</center></h1>\n",
    "\n",
    "[1] Hands-On ML Textbook Edition-2 2019\n",
    "\n",
    "[2] Clustering [Slides from UIowa](http://user.engineering.uiowa.edu/~ie_155/Lecture/K-means.pdf)\n",
    "\n",
    "[3] Clustering [Slides from MIT](http://www.mit.edu/~9.54/fall14/slides/Class13.pdf)\n",
    "\n",
    "[4] Clustering [Slides from CMU](https://www.cs.cmu.edu/~epxing/Class/10701/slides/clustering.pdf)\n",
    "\n",
    "[5] Clustering [David Sontag's Slides from NYU](http://people.csail.mit.edu/dsontag/courses/ml12/slides/lecture14.pdf) Slides adapted from Luke Zettlemoyer, Vibhav Gogate, Carlos Guestrin, Andrew Moore, Dan Klein\n",
    "\n",
    "[6] [Silhouette Loss Function Metric](https://platform.ai/blog/page/11/the-silhouette-loss-function-metric-learning-with-a-cluster-validity-index/)\n",
    "\n",
    "[7] https://towardsdatascience.com/image-segmentation-on-apache-spark-46164dd53c73\n",
    "\n",
    "[8] https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95\n",
    "\n",
    "[9] https://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/\n",
    "\n",
    "[10] [Machine Learning Specialization at Coursera - University of Washington](https://www.coursera.org/specializations/machine-learning)\n",
    "\n",
    "[11] https://stackoverflow.com/questions/26019584/understanding-concept-of-gaussian-mixture-models\n",
    "\n",
    "[12] https://pythonmachinelearning.pro/clustering-with-gaussian-mixture-models/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
